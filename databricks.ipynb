{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALrtrab-gfIs"
      },
      "source": [
        "IMPORT das bibiliotecas de pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zixlDiIHgfIu"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql import SparkSession\n",
        "import pyspark\n",
        "import json\n",
        "from pyspark.sql.functions import explode \n",
        "from pyspark.sql import functions\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "K7UbdFvMgfIu"
      },
      "source": [
        "<H4> Lendo um dataframe </H4>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aqzylh2ggfIu"
      },
      "outputs": [],
      "source": [
        "def chama_dataframe (path, tipo_arquivo = 'csv', header = True, inferSchema = True ):\n",
        "##tipo arquivo = Json, CSV, Parquet etc\n",
        "## se precisar pode por o .option('sep', ';')\\\n",
        "    try:\n",
        "        df = spark.read\\\n",
        "            .format(tipo_arquivo)\\\n",
        "            .option('header', header)\\\n",
        "            .option('inferSchema', inferSchema)\\\n",
        "            .load(path)\n",
        "    except:\n",
        "        print(\"error\")\n",
        "        \n",
        "    else:\n",
        "        print(\"Sucesso, Vamos prosseguir\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_EHAQOYUgfIv"
      },
      "source": [
        "Tratamentos de Colunas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uA3fL6NLgfIv"
      },
      "outputs": [],
      "source": [
        "#Criando uma função Lambda parar tratar o header da coluna, removendo o espaço vazio e colocando um underscore. \n",
        "Nova_Lista= list(map(lambda x: x.replace(\" \", \"_\"), df.columns))\n",
        "df = d.toDF(*Nova_Lista)\n",
        "#Abaixo todas as colunas com o nome maisculos\n",
        "df = df.toDF(*[c.upper() for c in df.columns])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_uoYNU_bgfIv"
      },
      "source": [
        "FUNÇÕES PRA TRATAMENTO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yam0A5GzgfIv"
      },
      "outputs": [],
      "source": [
        "#Trocar virgula por ponto OBS: você pode trocar por outras coisas tbm.\n",
        "def convert_virgula(parametro):\n",
        "    return df.withColumn(parametro, regexp_replace(parametro, \",\", \".\"))\n",
        "\n",
        "\n",
        "#Formatar a data \n",
        "def formatardata(parametro):\n",
        "    return df.withColumn(parametro, to_date(parametro, 'dd/MM/yyyy'))\n",
        "\n",
        "#Converter valor vazio em não informado OBS: Ou algo da sua escolha.\n",
        "def convertevazio(df1, substitui_por='Não informado'):\n",
        "    for i in df1.columns:\n",
        "        df1 = df1.withColumn(i , when(col(i).isNull() | col(i).isin(''),substitui_por).otherwise(col(i)))\n",
        "    return df1\n",
        "\n",
        "#Converter valor nulo em não informado, é o mesmo do de cima...\n",
        "def converternulo(df1, substitui_por='Não informado'):\n",
        "    for i in df1.columns:\n",
        "        df1 = df1.withColumn(i , when(col(i) == \"\", substitui_por).otherwise(col(i)))\n",
        "    return df1\n",
        "\n",
        "#Converter Valor null!\n",
        "def converternulo2(df1, substitui_por='Não informado'):\n",
        "    for i in df1.columns:\n",
        "        df1 = df1.na.fill(substitui_por)\n",
        "    return df1\n",
        "\n",
        "\n",
        "#Funções de Count\n",
        "#count nulos\n",
        "def countnull(df):\n",
        "    df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c)\n",
        "               for c in df.columns]).display()\n",
        "\n",
        "#count não nulos  \n",
        "def countnotnull(df):\n",
        "    df.select([count(when(isnan(c) | col(c).isNotNull(), c)).alias(c)\n",
        "               for c in df.columns]).display()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3eFHJBAgfIw"
      },
      "source": [
        "<H3>Tipo de funções do proprio spark</H3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#withColumn()\n",
        "\"Tratar valores internos da coluna\"\n",
        "#Eu prefiro utilizar listas, feito o unico que esta situado abaixo.\n",
        "df.withColumn('nomedacoluna', when(col('nomedacoluna').isin(unico), 'valorquemudarar').otherwise(col('nomedacoluna')))\n",
        "    \n",
        "#withColumnRenamed()\n",
        "\"Trata o nome das colunas\"\n",
        "df.withColumnRenamed(\"Antigo-nome\", 'Novo-nome')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pdQB08GRleom"
      },
      "outputs": [],
      "source": [
        "### se quiser tratar dados pelo python\n",
        "class Transform_lista:\n",
        "  \n",
        "    def coluna_to_lista (dataframe, str_coluna):\n",
        "      #transforma um dataframe em uma lista\n",
        "        list_cidade = dataframe.select([str_coluna]).rdd.flatMap(lambda x: x).collect()\n",
        "        return list_cidade\n",
        "    \n",
        "    def lista_minuscula (lista): \n",
        "        #cria uma lista com letra minuscula\n",
        "        lista_l = [str(i).lstrip() for i in lista]\n",
        "        return lista_l\n",
        "    \n",
        "    def lista_unicos (lista, local=0, acento = False):\n",
        "        #realiza a transformação tirando assentos e mantém somente um valor, em ordem alfabética\n",
        "        cid = []\n",
        "        for i in lista:\n",
        "            if acento:\n",
        "                a = unidecode(i)\n",
        "            elif local != 0:\n",
        "                a=i[local]\n",
        "            else:\n",
        "                a = i\n",
        "            \n",
        "            if a not in cid:\n",
        "                cid.append(a)\n",
        "        cid = sorted(cid)\n",
        "        return cid\n",
        "      \n",
        "    def lista_eID(lista):\n",
        "        #Cria uma lista com listas com id\n",
        "        return [[i, lista[i]] for i in range(len(lista))]\n",
        "    \n",
        "    def del_null (lista):\n",
        "        #deleta valores nulos de uma lista\n",
        "        l_fix=[]\n",
        "        for i in lista:\n",
        "            linha = []\n",
        "            for j in i:\n",
        "                if j != None:\n",
        "                    linha.append(j)\n",
        "            l_fix.append(linha)\n",
        "        return l_fix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AyAqTkAynn8r"
      },
      "source": [
        "Andando pelo Keyvault"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yFFhF8dDnWES"
      },
      "outputs": [],
      "source": [
        "dbutils.secrets.listScopes() #lista todos os escopos do kv\n",
        "\n",
        "dbutils.secrets.list('<escopo>') #lista as keys do escopo\n",
        "\n",
        "key = dbutils.secrets.get(scope='<escopo>',key='<nome-chave>') #pega um valor de uma chave\n",
        "\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ingestão"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "def ingestion_table(table_name):\n",
        "    query = import_query(f\"sql/{table_name}.sql\")\n",
        "      \n",
        "    (spark.sql(query)\\\n",
        "    .coalesce(1)\\\n",
        "    .write\n",
        "    .mode('overwrite')\n",
        "    .format(\"delta\")\n",
        "    .option(\"overwriteSchema\", True)\n",
        "    .saveAsTable(f\"silver.{table_name}\"))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4 (tags/v3.10.4:9d38120, Mar 23 2022, 23:13:41) [MSC v.1929 64 bit (AMD64)]"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "cabb79eba6a7bbc86f68b9d7ec45eb843010dd03cdb24ac98d3af13e82d22868"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
