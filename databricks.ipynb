{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALrtrab-gfIs"
      },
      "source": [
        "IMPORT das bibiliotecas de pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zixlDiIHgfIu"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql import SparkSession\n",
        "import pyspark\n",
        "import json\n",
        "from pyspark.sql.functions import explode \n",
        "from pyspark.sql import functions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7UbdFvMgfIu"
      },
      "source": [
        "<H2> Chamando o dataframe no blob </H2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aqzylh2ggfIu"
      },
      "outputs": [],
      "source": [
        "def chama_dataframe (path, tipo_arquivo = 'csv', header = True, inferSchema = True ):\n",
        "##tipo arquivo = Json, CSV, Parquet etc\n",
        "## se precisar pode por o .option('sep', ';')\\\n",
        "    try:\n",
        "        df = spark.read\\\n",
        "            .format(tipo_arquivo)\\\n",
        "            .option('header', header)\\\n",
        "            .option('inferSchema', inferSchema)\\\n",
        "            .load(path)\n",
        "    except:\n",
        "        print(\"error\")\n",
        "        \n",
        "    else:\n",
        "        print(\"Sucesso, Vamos prosseguir\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_EHAQOYUgfIv"
      },
      "source": [
        "Tratamentos de Colunas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uA3fL6NLgfIv"
      },
      "outputs": [],
      "source": [
        "#Criando uma função Lambda parar tratar o header da coluna, removendo o espaço vazio e colocando um underscore. \n",
        "Nova_Lista= list(map(lambda x: x.replace(\" \", \"_\"), df.columns))\n",
        "df = d.toDF(*Nova_Lista)\n",
        "#Abaixo todas as colunas com o nome maisculos\n",
        "df = df.toDF(*[c.upper() for c in df.columns])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_uoYNU_bgfIv"
      },
      "source": [
        "FUNÇÕES PRA TRATAMENTO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yam0A5GzgfIv"
      },
      "outputs": [],
      "source": [
        "#Trocar virgula por ponto OBS: você pode trocar por outras coisas tbm.\n",
        "def convert_virgula(parametro):\n",
        "    return df.withColumn(parametro, regexp_replace(parametro, \",\", \".\"))\n",
        "\n",
        "\n",
        "#Formatar a data \n",
        "def formatardata(parametro):\n",
        "    return df.withColumn(parametro, to_date(parametro, 'dd/MM/yyyy'))\n",
        "\n",
        "#Converter valor vazio em não informado OBS: Ou algo da sua escolha.\n",
        "def convertevazio(df1, substitui_por='Não informado'):\n",
        "    for i in df1.columns:\n",
        "        df1 = df1.withColumn(i , when(col(i).isNull() | col(i).isin(''),substitui_por).otherwise(col(i)))\n",
        "    return df1\n",
        "\n",
        "#Converter valor nulo em não informado, é o mesmo do de cima...\n",
        "def converternulo(df1, substitui_por='Não informado'):\n",
        "    for i in df1.columns:\n",
        "        df1 = df1.withColumn(i , when(col(i) == \"\", substitui_por).otherwise(col(i)))\n",
        "    return df1\n",
        "\n",
        "#Converter Valor null!\n",
        "def converternulo2(df1, substitui_por='Não informado'):\n",
        "    for i in df1.columns:\n",
        "        df1 = df1.na.fill(substitui_por)\n",
        "    return df1\n",
        "\n",
        "\n",
        "#Funções de Count\n",
        "#count nulos\n",
        "def countnull(df):\n",
        "    df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c)\n",
        "               for c in df.columns]).display()\n",
        "\n",
        "#count não nulos  \n",
        "def countnotnull(df):\n",
        "    df.select([count(when(isnan(c) | col(c).isNotNull(), c)).alias(c)\n",
        "               for c in df.columns]).display()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3eFHJBAgfIw"
      },
      "source": [
        "<H3>Tipo de funções do proprio spark</H3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#withColumn()\n",
        "\"Tratar valores internos da coluna\"\n",
        "#Eu prefiro utilizar listas, feito o unico que esta situado abaixo.\n",
        "df.withColumn('nomedacoluna', when(col('nomedacoluna').isin(unico), 'valorquemudarar').otherwise(col('nomedacoluna')))\n",
        "    \n",
        "#withColumnRenamed()\n",
        "\"Trata o nome das colunas\"\n",
        "df.withColumnRenamed(\"Antigo-nome\", 'Novo-nome')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h_oC1oytgfIw"
      },
      "outputs": [],
      "source": [
        "class Conexao:\n",
        "#classe conexão com funções para conectar ao banco SQL  \n",
        "\n",
        "     #Informações para a conexão\n",
        "    jdbcHostname = '<NOMEDOHOST>.database.windows.net'\n",
        "    jdbcDatabase = '<NOME DO DB>'\n",
        "    jdbcPort = 1433 #numero da porta\n",
        "    jdbcUsername = '<USERNAME>'\n",
        "    jdbcPassword = dbutils.secrets.get(scope = '<nome do escopo do KV>', key = '<nome da chave>')\n",
        "    jdbcUrl = f'jdbc:sqlserver://{jdbcHostname}:{jdbcPort};database={jdbcDatabase}'\n",
        "    \n",
        "    def conectar(jdbcUsername=jdbcUsername, jdbcPassword = jdbcPassword ):\n",
        "        #retorna informações de conexão com o SQL\n",
        "        connectionProperties = {\n",
        "          \"user\" : jdbcUsername,\n",
        "          \"password\" : jdbcPassword,\n",
        "          \"driver\" : \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n",
        "            }\n",
        "        return connectionProperties\n",
        "        \n",
        "    def salvar(var, nome_tabela, schema, jdbcUrl=jdbcUrl,jdbcUsername = jdbcUsername, jdbcPassword = jdbcPassword):\n",
        "        #salva um dataframe (var) com o nome schema.nome_tabela no banco de dados\n",
        "        var.write.format('jdbc')\\\n",
        "                        .mode('overwrite')\\\n",
        "                        .option('url', jdbcUrl)\\\n",
        "                        .option('dbtable',  f'{schema}.{nome_tabela}' )\\\n",
        "                        .option(\"user\", jdbcUsername) \\\n",
        "                        .option(\"password\", jdbcPassword) \\\n",
        "                        .save()  \n",
        "        \n",
        "    def url_sql (jdbcUrl = jdbcUrl):\n",
        "        #retorna somente a URL do banco, caso precise\n",
        "        return jdbcUrl\n",
        "    \n",
        "    def retorna_df (nome_tabela_sql):\n",
        "        #falta testar, deve funcionar\n",
        "        conexao = Conexao.conectar()\n",
        "        df = spark.read.jdbc(url=jdbcUrl, table=nome_tabela_sql, properties=conexao)\n",
        "        return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pdQB08GRleom"
      },
      "outputs": [],
      "source": [
        "### se quiser tratar dados pelo python\n",
        "class Transform_lista:\n",
        "  \n",
        "    def coluna_to_lista (dataframe, str_coluna):\n",
        "      #transforma um dataframe em uma lista\n",
        "        list_cidade = dataframe.select([str_coluna]).rdd.flatMap(lambda x: x).collect()\n",
        "        return list_cidade\n",
        "    \n",
        "    def lista_minuscula (lista): \n",
        "        #cria uma lista com letra minuscula\n",
        "        lista_l = [str(i).lstrip() for i in lista]\n",
        "        return lista_l\n",
        "    \n",
        "    def lista_unicos (lista, local=0, acento = False):\n",
        "        #realiza a transformação tirando assentos e mantém somente um valor, em ordem alfabética\n",
        "        cid = []\n",
        "        for i in lista:\n",
        "            if acento:\n",
        "                a = unidecode(i)\n",
        "            elif local != 0:\n",
        "                a=i[local]\n",
        "            else:\n",
        "                a = i\n",
        "            \n",
        "            if a not in cid:\n",
        "                cid.append(a)\n",
        "        cid = sorted(cid)\n",
        "        return cid\n",
        "      \n",
        "    def lista_eID(lista):\n",
        "        #Cria uma lista com listas com id\n",
        "        return [[i, lista[i]] for i in range(len(lista))]\n",
        "    \n",
        "    def del_null (lista):\n",
        "        #deleta valores nulos de uma lista\n",
        "        l_fix=[]\n",
        "        for i in lista:\n",
        "            linha = []\n",
        "            for j in i:\n",
        "                if j != None:\n",
        "                    linha.append(j)\n",
        "            l_fix.append(linha)\n",
        "        return l_fix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AyAqTkAynn8r"
      },
      "source": [
        "Andando pelo Keyvault"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yFFhF8dDnWES"
      },
      "outputs": [],
      "source": [
        "dbutils.secrets.listScopes() #lista todos os escopos do kv\n",
        "\n",
        "dbutils.secrets.list('<escopo>') #lista as keys do escopo\n",
        "\n",
        "key = dbutils.secrets.get(scope='<escopo>',key='<nome-chave>') #pega um valor de uma chave\n",
        "\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "SwbtzqBvo1Lv"
      },
      "source": [
        "mount e unmount no Azure/S3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3PCDmCRgnycR"
      },
      "outputs": [],
      "source": [
        "dbutils.fs.mount(\n",
        "   source = f\"wasbs://{nome_container}@{storage}.blob.core.windows.net\",\n",
        "   mount_point=f\"/mnt/{nome_container}\",\n",
        "   extra_configs={f\"fs.azure.account.key.{storage}.blob.core.windows.net\":key})\n",
        "\n",
        "dbutils.fs.unmount(f\"/mnt/{nome_container}\")\n",
        "\n",
        "\n",
        "#S3 \n",
        "aws_bucket_name = \"<aws-bucket-name>\"\n",
        "mount_name = \"<mount-name>\"\n",
        "dbutils.fs.mount(f\"s3a://{aws_bucket_name}\", f\"/mnt/{mount_name}\")\n",
        "display(dbutils.fs.ls(f\"/mnt/{mount_name}\"))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ingestão"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "def ingestion_table(table_name):\n",
        "    query = import_query(f\"sql/{table_name}.sql\")\n",
        "      \n",
        "    (spark.sql(query)\\\n",
        "    .coalesce(1)\\\n",
        "    .write\n",
        "    .mode('overwrite')\n",
        "    .format(\"delta\")\n",
        "    .option(\"overwriteSchema\", True)\n",
        "    .saveAsTable(f\"silver.{table_name}\"))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.8 64-bit (microsoft store)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "cf9431632eb10c3814cb567841a595cfe60e8efa8d9004c7d93aa74a8a744504"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
